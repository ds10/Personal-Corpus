setwd("~/RStudio")
fileName <- '2006.txt'
readChar(fileName, file.info(fileName)$size)
2006<-readChar(fileName, file.info(fileName)$size)
f2006<-readChar(fileName, file.info(fileName)$size)
gsub("<.*/>","",f2006)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
txtTdmBi <- TermDocumentMatrix(f2006, control = list(tokenize = BigramTokenizer))
library(tm)
library(RWeka)
txtTdmBi <- TermDocumentMatrix(f2006, control = list(tokenize = BigramTokenizer))
data(crude)
corpus <-Corpus(DirSource(“blogs”))
corpus <-Corpus(DirSource("blogs"))
fix(corpus)
fix(corpus)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
txtTdmBi <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
inspect(txtTdmBi)
inspect(txtTdmBi)
inspect(corpus)
inspect(crude)
txtTdmBi <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
inspect(txtTdmBi)
txtTdmBi <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
txtTdmBi <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
inspect(txtTdmBi)
txtTdmBi <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
inspect(crude)
inspect(txtTdmBi)
fix(corpus)
fix(crude)
fix(corpus)
fix(txtTdmBi)
userTimeline<-(me)
me <- getUser("paddytherabbit")
library(twitteR)
library(ROAuth)
library(RCurl)
requestURL <-  "https://api.twitter.com/oauth/request_token"
accessURL =    "https://api.twitter.com/oauth/access_token"
authURL =      "https://api.twitter.com/oauth/authorize"
consumerKey =   "70B0eTduZnUsGbHsCXRiw"
consumerSecret = "xZnJBfycBqy5cVTQUL8AGpsxXJqybkaVJKS7duoY"
twitCred <- OAuthFactory$new(consumerKey=consumerKey,
consumerSecret=consumerSecret,
requestURL=requestURL,
accessURL=accessURL,
authURL=authURL)
#download.file(url="http://curl.haxx.se/ca/cacert.pem",
#              destfile="cacert.pem")
twitCred$handshake()
twitCred$handshake()
# works, in future I can just
load("twitteR_credentials")
me <- getUser("paddytherabbit")
registerTwitterOAuth(twitCred)
twitCred$handshake()
save(list="twitCred", file="twitteR_credentials")
me <- getUser("paddytherabbit")
registerTwitterOAuth(twitCred)
me <- getUser("paddytherabbit")
userTimeline<-(me)
fix(userTimeline)
me <- getUser("paddytherabbit")
userTimeline<-(me)
userTimeline<-("paddytherabbit")
fix(userTimeline)
userTimeline("paddytherabbit")
userTimeline(me)
inspect(userTimeline)
view(userTimeline)
cat(userTimeline)
userTimeline<-userTimeline(me)
fix(userTimeline)
fix(userTimeline)
userTimeline<-userTimeline(me, 3200)
library(twitteR)
library(ROAuth)
library(RCurl)
requestURL <-  "https://api.twitter.com/oauth/request_token"
accessURL =    "https://api.twitter.com/oauth/access_token"
authURL =      "https://api.twitter.com/oauth/authorize"
consumerKey =   "70B0eTduZnUsGbHsCXRiw"
consumerSecret = "xZnJBfycBqy5cVTQUL8AGpsxXJqybkaVJKS7duoY"
twitCred <- OAuthFactory$new(consumerKey=consumerKey,
consumerSecret=consumerSecret,
requestURL=requestURL,
accessURL=accessURL,
authURL=authURL)
fix(userTimeline)
test<-data.frame(userTimeline)
twListToDF
twListToDF()
twListToDF()
tw.df=do.call(‘rbind’,lapply(userTimeline, as.data.frame))
tw.df=do.call("rbind",lapply(userTimeline, as.data.frame))
?twListToDF
?twListToDF
library(twitteR)
tw.df=do.call("rbind",lapply(userTimeline, as.data.frame))
View(tw.df)
require(tm)
a <- Corpus(VectorSource(df$text)) # create corpus object
View(tw.df)
a <- Corpus(VectorSource(tw.df$text)) # create corpus object
a <- tm_map(a, tolower) # convert all text to lower case
a <- tm_map(a, removePunctuation)
a <- tm_map(a, removeNumbers)
a <- tm_map(a, removeWords, stopwords("english")) # this list needs to be edited and this function repeated a few times to remove high frequency context specific words with no semantic value
require(rJava) # needed for stemming function
library(Snowball) # also needed for stemming function
a <- tm_map(a, stemDocument, language = "english") # converts terms to tokens
a.tdm <- TermDocumentMatrix(a, control = list(minWordLength = 3)) # create a term document matrix, keeping only tokens longer than three characters, since shorter tokens are very hard to interpret
inspect(a.tdm[1:10,1:10]) # have a quick look at the term document matrix
findFreqTerms(a.tdm, lowfreq=30) # have a look at common words, in this case, those that appear at least 30 times, good to get high freq words and add to stopword list and re-make the dtm, in this case add aaa, panel, session
findAssocs(a.tdm, 'science', 0.30) # find associated words and strength of the common words. I repeated this function for the ten most frequent words.
findFreqTerms(a.tdm, lowfreq=30) # have a look at common words, in this case, those that appear at least 30 times, good to get high freq words and add to stopword list and re-make the dtm, in this case add aaa, panel, session
findFreqTerms(a.tdm, lowfreq=10) # have a look at common words, in this case, those that appear at least 30 times, good to get high freq words and add to stopword list and re-make the dtm, in this case add aaa, panel, session
findFreqTerms(a.tdm, lowfreq=5) # have a look at common words, in this case, those that appear at least 30 times, good to get high freq words and add to stopword list and re-make the dtm, in this case add aaa, panel, session
findAssocs(a.tdm, 'science', 0.30) # find associated words and strength of the common words. I repeated this function for the ten most frequent words.
findAssocs(a.tdm, 'post', 0.30) # find associated words and strength of the common words. I repeated this function for the ten most frequent words.
require(stringr)
require(ggplot2)
require(grid)
df$link <- sapply(df$text,function(tweet) str_extract(tweet,("http[[:print:]]+"))) # creates new field and extracts the links contained in the tweet
df$link <- sapply(df$text,function(tweet) str_extract(tweet,"http[[:print:]]{16}")) # limits to just 16 characters after http so I just get the shortened link.
countlink <- data.frame(URL = as.character(unlist(dimnames(sort(table(df$link))))), N = sort(table(df$link))) # get frequencies of each link and put in rank order
rownames(countlink) <- NULL # remove rownames
countlinkSub <- subset(countlink, N>2) # subset of just links appearing more than twice
# plot to see distribution of links
ggplot(countlinkSub, aes(reorder(URL, N), N)) +
xlab("URL") +
ylab("Number of messages containing the URL")+
geom_point() +
coord_flip() +
theme_bw()  +
theme(axis.title.x = element_text(vjust = -0.5, size = 14)) +
theme(axis.title.y = element_text(size = 14, angle=90)) +
theme(plot.margin = unit(c(1,1,2,2), "lines"))
require(topicmodels)
best.model <- lapply(seq(2, 50, by = 1), function(d){LDA(a.tdm.sp.t.tdif, d)}) # this will make a topic model for every number of topics between 2 and 50... it will take some time!
best.model.logLik <- as.data.frame(as.matrix(lapply(best.model, logLik)))  # this will produce a list of logLiks for each model...
# plot the distribution of logliklihoods by topic
best.model.logLik.df <- data.frame(topics=c(2:50), LL = as.numeric(as.matrix(best.model.logLik)))
ggplot(best.model.logLik.df, aes(x = topics, y = LL)) +
require(topicmodels)
best.model <- lapply(seq(2, 50, by = 1), function(d){LDA(a.tdm.sp.t.tdif, d)}) # this will make a topic model for every number of topics between 2 and 50... it will take some time!
require(slam)
a.tdm.sp.t <- t(a.tdm.sp) # transpose term document matrix, necessary for the next steps using mean term frequency-inverse document frequency (tf-idf) to select the vocabulary for topic modeling
a.tdm.sp <- removeSparseTerms(a.tdm, sparse=0.989)  # I found I had to iterate over this to ensure the tdm doesn't get too small... for example: 0.990 nrow=88, 0.989, nrow=67, 0.985, nrow=37, 0.98 nrow=23, 0.95 nrow=6
a.tdm.sp.df <- as.data.frame(inspect(a.tdm.sp)) # convert document term matrix to data frame
nrow(a.tdm.sp.df) # check to see how many words we're left with after removing sparse terms
# this analysis is based on http://www.statmethods.net/advstats/cluster.html
# scale and transpose data for cluster analysis
a.tdm.sp.df.sc.t <- t(scale(a.tdm.sp.df))
require(pvclust)
fit <- pvclust(a.tdm.sp.df.sc.t, method.hclust = "average", method.dist = "correlation", nboot = 10) # this method may take a few hours the bootstraping, you can reduce the nboot value for a quicker result
plot(fit, cex = 1.5, cex.pv = 1.2, col.pv = c(1,0,0), main="", xlab="", sub="")  # draw the dendrogram
require(slam)
a.tdm.sp.t <- t(a.tdm.sp) # transpose term document matrix, necessary for the next steps using mean term frequency-inverse document frequency (tf-idf) to select the vocabulary for topic modeling
summary(col_sums(a.tdm.sp.t)) # check median...
mean_term_tfidf <- tapply(a.tdm.sp.t$v/row_sums(a.tdm.sp.t)[a.tdm.sp.t$i], a.tdm.sp.t$j,mean) * log2(nDocs(a.tdm.sp.t)/col_sums(a.tdm.sp.t>0)) # calculate tf-idf values
# CAUTION: Note that Hugh J. Devlin has pointed out that this tf-idf is not the conventional computation because the term document matrix has been transposed
# for the usual tf-idf computation, skip the transpose operation on line 330
summary(mean_term_tfidf) # check median... note value for next line...
a.tdm.sp.t.tdif <- a.tdm.sp.t[,mean_term_tfidf>=1.0] # keep only those terms that are slightly less frequent that the median
a.tdm.sp.t.tdif <- a.tdm.sp.t[row_sums(a.tdm.sp.t) > 0, ]
summary(col_sums(a.tdm.sp.t.tdif)) # have a look
# investigate the URLs contained in the Twitter messages
# Before going right into generating the topic model and analysing the output, we need to decide on the number of topics that the model should use
# Here's a function to loop over different topic numbers, get the log liklihood of the model for each topic number and plot it so we can pick the best one
# The best number of topics is the one with the highest log liklihood value.
require(topicmodels)
best.model <- lapply(seq(2, 50, by = 1), function(d){LDA(a.tdm.sp.t.tdif, d)}) # this will make a topic model for every number of topics between 2 and 50... it will take some time!
best.model.logLik <- as.data.frame(as.matrix(lapply(best.model, logLik)))  # this will produce a list of logLiks for each model...
# plot the distribution of logliklihoods by topic
best.model.logLik.df <- data.frame(topics=c(2:50), LL = as.numeric(as.matrix(best.model.logLik)))
ggplot(best.model.logLik.df, aes(x = topics, y = LL)) +
xlab("Number of topics") +
ylab("Log likelihood of the model") +
geom_line() +
theme_bw()  +
opts(axis.title.x = theme_text(vjust = -0.5, size = 14)) +
opts(axis.title.y=theme_text(size = 14, angle=90, vjust= -0.25)) +
opts(plot.margin = unit(c(1,1,2,2), "lines"))  # plot nicely the ggsave(file = "model_LL_per_topic_number.pdf") # export the plot to a PDF file
# it's not easy to see exactly which topic number has the highest LL, so let's look at the data...
best.model.logLik.df.sort <- best.model.logLik.df[order(-best.model.logLik.df$LL), ] # sort to find out which number of topics has the highest loglik, in this case 23 topics.
best.model.logLik.df.sort # have a look to see what's at the top of the list, the one with the highest score
lda <- LDA(a.tdm.sp.t.tdif,23) # generate a LDA model with 23 topics, as found to be optimum
get_terms(lda, 5) # get keywords for each topic, just for a quick look
get_topics(lda, 5) # gets topic numbers per document
lda_topics<-get_topics(lda, 5)
beta <- lda@beta # create object containing parameters of the word distribution for each topic
gamma <- lda@gamma # create object containing posterior topic distribution for each document
terms <- lda@terms # create object containing terms (words) that can be used to line up with beta and gamma
colnames(beta) <- terms # puts the terms (or words) as the column names for the topic weights.
id <- t(apply(beta, 1, order)) # order the beta values
beta_ranked <- lapply(1:nrow(id),function(i)beta[i,id[i,]])  # gives table of words per topic with words ranked in order of beta values. Useful for determining the most important words per topic
source("RedditScraper.R")
redditScrape('ukpolitics','week'   )
setwd("~/Desktop/RStudio")
source("RedditScraper.R")
setwd("~/Desktop/RStudio/Personal Corpus/R")
source("RedditScraper.R")
redditScrape('ukpolitics','week'   )
pwd
pwd()
wd()
getwd()
source("RedditTopicgeneratorScraper.R")
redditScrape('ukpolitics','week'   )
source("RedditTopicgeneratorScraper.R")
redditScrape('ukpolitics','week'   )
source("RedditTopicgeneratorScraper.R")
redditScrape('ukpolitics','week'   )
source("RedditTopicgeneratorScraper.R")
redditScrape('ukpolitics','week'   )
getwd()
source("RedditTopicgeneratorScraper.R")
redditScrape('ukpolitics','week'   )
subred ="ukpolitics";
if (subred == 'allTop') {
url <- paste('http://www.reddit.com/top/?sort=top&t=', time, sep = "")
} else {
url <- paste("http://www.reddit.com/r/", subred, "/top/?sort=top&t=", time, sep = "")
}
subred<-"ukpolitics";
if (subred == 'allTop') {
url <- paste('http://www.reddit.com/top/?sort=top&t=', time, sep = "")
} else {
url <- paste("http://www.reddit.com/r/", subred, "/top/?sort=top&t=", time, sep = "")
}
url <- paste('http://www.reddit.com/top/?sort=top&t=', time, sep = "")
if (subred == 'allTop') {
url <- paste('http://www.reddit.com/top/?sort=top&t=', time, sep = "")
} else {
url <- paste("http://www.reddit.com/r/", subred, "/top/?sort=top&t=", time, sep = "")
}
time<-"year"
url <- paste("http://www.reddit.com/r/", subred, "/top/?sort=top&t=", time, sep = "")
links <- xpathSApply(doc, "//a/@href")
comments <- grep("comments", links)
comLinks <- links[comments]
comments <- grep('reddit.com', comLinks)
comLinks <- comLinks[comments]
links <- xpathSApply(doc, "//a/@href")
require(XML)
require(RCurl)
require(RColorBrewer)
require(wordcloud)
links <- xpathSApply(doc, "//a/@href")
doc <- htmlParse(url)
links <- xpathSApply(doc, "//a/@href")
comments <- grep("comments", links)
comLinks <- links[comments]
comments <- grep('reddit.com', comLinks)
comLinks <- comLinks[comments]
#######################################################
#  3. Scrape the pages
#  This will scrape a page and put it in to
#  an R list object
textList <- as.list(rep(as.character(""), length(comLinks)))
docs <- getURL(comLinks)
fix(textList)
for (i in 1:length(textList)) {
submitLine <- grep("submitted [0-9]", textList[[i]])
textList[[i]] <- textList[[i]][{(submitLine[1] + 1):(length(textList[[i]])-10)}]
}
textList <- as.list(rep(as.character(""), length(comLinks)))
docs <- getURL(comLinks)
for (i in 1:length(docs)) {
textList[[i]] <- htmlParse(docs[i], asText = TRUE)
textList[[i]] <- xpathSApply(textList[[i]], "//p", xmlValue)
}
textList <- as.list(rep(as.character(""), length(comLinks)))
docs <- getURL(comLinks)
for (i in 1:length(docs)) {
textList[[i]] <- htmlParse(docs[i], asText = TRUE)
textList[[i]] <- xpathSApply(textList[[i]], "//p", xmlValue)
}
#######################################################
#  4. Clean up the text.
# Remove the submitted lines and lines at the end of each page
for (i in 1:length(textList)) {
submitLine <- grep("submitted [0-9]", textList[[i]])
textList[[i]] <- textList[[i]][{(submitLine[1] + 1):(length(textList[[i]])-10)}]
}
# Removing lines capturing user and points, etc.
# Yes, there could be fewer grep calls, but this made it
# easier to keep track of what was going on.
for (i in 1:length(textList)) {
grep('points 1 minute ago', textList[[i]]) -> nameLines1
grep('points [0-9] minutes ago', textList[[i]]) -> nameLines2
grep('points [0-9][0-9] minutes ago', textList[[i]]) -> nameLines3
grep("points 1 hour ago", textList[[i]]) -> nameLines4
grep("points [0-9] hours ago", textList[[i]]) -> nameLines5
grep("points [0-9][0-9] hours ago", textList[[i]]) -> nameLines6
grep('points 1 day ago', textList[[i]]) -> nameLines7
grep('points [0-9] days ago', textList[[i]]) -> nameLines8
grep('points [0-9][0-9] days ago', textList[[i]]) -> nameLines9
grep('points 1 month ago', textList[[i]]) -> nameLines10
grep('points [0-9] months ago', textList[[i]]) -> nameLines11
grep('points [0-9][0-9] months ago', textList[[i]]) -> nameLines12
allLines <- c(nameLines1, nameLines2, nameLines3, nameLines4,
nameLines5, nameLines6, nameLines7, nameLines8, nameLines9,
nameLines10, nameLines11, nameLines12)
textList[[i]] <- textList[[i]][-allLines]
textList[[i]] <- textList[[i]][textList[[i]]!=""]
textList[[i]] <- tolower(textList[[i]])
}
# Let's simplify our list. Could have been done earlier, but so it goes.
allText <- unlist(textList)
fix(comLinks)
fix(allText)
documents <- data.frame( text = allText, stringsAsFactors=FALSE)
View(documents)
documents$id <- Sys.time() + 30 * (seq_len(nrow(documents))-1)
View(documents)
allText <- gsub("https?://[[:alnum:][:punct:]]+", "", allText)
allText <- gsub("[,.!?\"]", "", allText)
allText <- strsplit(allText, "\\W+", perl=TRUE)
documents <- data.frame( text = allText, stringsAsFactors=FALSE)
documents$id <- Sys.time() + 30 * (seq_len(nrow(documents))-1)
View(documents)
mallet.instances <- mallet.import( documents$text ,  documents$text , "stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
require(mallet)
mallet.instances <- mallet.import( documents$text ,  documents$text , "stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
## Create a topic trainer object.
n.topics <- 30
topic.model <- MalletLDA(n.topics)
#loading the documents
topic.model$loadDocuments(mallet.instances)
## Get the vocabulary, and some statistics about word frequencies.
##  These may be useful in further curating the stopword list.
vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)
## Optimize hyperparameters every 20 iterations,
##  after 50 burn-in iterations.
topic.model$setAlphaOptimization(20, 50)
## Now train a model. Note that hyperparameter optimization is on, by default.
##  We can specify the number of iterations. Here we'll use a large-ish round number.
topic.model$train(200)
## NEW: run through a few iterations where we pick the best topic for each token,
##  rather than sampling from the posterior distribution.
topic.model$maximize(10)
## Get the probability of topics in documents and the probability of words in topics.
## By default, these functions return raw word counts. Here we want probabilities,
##  so we normalize, and add "smoothing" so that nothing has exactly 0 probability.
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)
# from http://www.cs.princeton.edu/~mimno/R/clustertrees.R
## transpose and normalize the doc topics
topic.docs <- t(doc.topics)
topic.docs <- topic.docs / rowSums(topic.docs)
## Get a vector containing short names for the topics
topics.labels <- rep("", n.topics)
for (topic in 1:n.topics) topics.labels[topic] <- paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=" ")
# have a look at keywords for each topic
topics.labels
fix(topics.labels)
