df<-rbind(df,data.frame(username = tmp, location = location)
}
foreach(i=1:nrow(tweets.df)) %do% {
x<-tweets.df[i,][11]
tmp = getUser(x)
location(tmp)
df<-rbind(df,data.frame(username = tmp, location = location)
}
x<-tweets.df[i,][11]
tmp = getUser(x)
location(tmp)
df<-rbind(df,data.frame(username = tmp, location = location))
foreach(i=1:nrow(tweets.df)) %do% {
x<-tweets.df[i,][11]
tmp = toString(getUser(x))
location = toString(location(tmp))
df<-rbind(df,data.frame(username = tmp, location = location))
}
View(df)
foreach(i=1:nrow(tweets.df)) %do% {
x<-tweets.df[i,][11]
tmp = toString(getUser(x))
location = toString(location(tmp))
df<-rbind(df,data.frame(username = tmp, location = location))
}
df <- data.frame(username=character(),
location=enviroment()
)
foreach(i=1:nrow(tweets.df)) %do% {
x<-tweets.df[i,][11]
tmp = toString(getUser(x))
location = toString(location(tmp))
df<-rbind(df,data.frame(username = tmp, location = location))
}
df <- data.frame(username,
location
)
df <- data.frame(
)
foreach(i=1:nrow(tweets.df)) %do% {
x<-tweets.df[i,][11]
tmp = toString(getUser(x))
location = toString(location(tmp))
df<-rbind(df,data.frame(username = tmp, location = location))
}
View(tweets.df)
fix(answer)
View(tweets.df)
View(df)
View(x)
read.csv(pubs.csv)
setwd("~/Desktop/RStudio/Personal Corpus/R")
read.csv(pubs.csv)
read.csv("pubs.csv")
read.csv("pubs.csv")
read.csv("pubs.csv")
read.csv("pubs.csv")
read.csv("pubs.csv")
read.csv("pubs.csv")
library(maptools)
world<-readShapePoly("yourworldshapefile")
install.packages("maptools")
library(maptools)
world<-readShapePoly("yourworldshapefile")
gpclibPermit()
gpclibPermit(TRUE)
gpclibPermit()
world<-readShapePoly("shapeworld.shp")
setwd("~/Desktop/RStudio/Personal Corpus/R")
world<-readShapePoly("shapeworld.shp")
world<-readShapePoly("shapeworld.shp")
world<-readShapePoly("sw")
world<-readShapePoly("sw.shx")
gpclibpermit
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='personalcorpus', host='localhost')
query<-paste('SELECT store,site,date FROM store')
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
doc <- Corpus(VectorSource(data.frame$store))
summary(doc)
doc.corpus <-tm_map(doc, removeWords, stopwords("english")) #removes stopwords
doc.corpus <-tm_map(doc.corpus, stripWhitespace) #removes stopwords
doc.corpus <-tm_map(doc.corpus, tolower)
doc.corpus <-tm_map(doc.corpus, removeNumbers)
doc.corpus <-tm_map(doc.corpus, removePunctuation)
#This script analayses two wordpress blogs that are stored in a mysqldatabase. It's taken script one from settings and script two is set here:
source('~/.active-rstudio-document')
install.packages("lsa")
source('~/.active-rstudio-document')
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='personalcorpus', host='localhost')
query<-paste('SELECT store,site,date FROM store')
external.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
doc <- Corpus(VectorSource(external.frame$store))
summary(doc)
externaldoc.corpus <-tm_map(doc, removeWords, stopwords("english")) #removes stopwords
externaldoc.corpus <-tm_map(externaldoc.corpus, stripWhitespace) #removes stopwords
externaldoc.corpus <-tm_map(externaldoc.corpus, tolower)
externaldoc.corpus <-tm_map(externaldoc.corpus, removeNumbers)
externaldoc.corpus <-tm_map(externaldoc.corpus, removePunctuation)
View(external.frame)
td.mat <- as.matrix(TermDocumentMatrix(corpus))
dist.mat <- dist(t(as.matrix(td.mat)))
dist.mat  # check distance matrix
td.mat <- as.matrix(TermDocumentMatrix(externaldoc.corpus ))
dist.mat <- dist(t(as.matrix(td.mat)))
dist.mat  # check distance matrix
fit <- cmdscale(dist.mat, eig = TRUE, k = 2)
points <- data.frame(x = fit$points[, 1], y = fit$points[, 2])
ggplot(points, aes(x = x, y = y)) + geom_point(data = points, aes(x = x, y = y,
color = df$view)) + geom_text(data = points, aes(x = x, y = y - 0.2, label = row.names(df)))
color = df$view)) + geom_text(data = points, aes(x = x, y = y - 0.2, label = row.names(df)))
# 1. Prepare mock data
text <- c("transporting food by cars will cause global warming. so we should go local.",
"we should try to convince our parents to stop using cars because it will cause global warming.",
"some food, such as mongo, requires a warm weather to grow. so they have to be transported to canada.",
"a typical electronic circuit can be built with a battery, a bulb, and a switch.",
"electricity flows from batteries to the bulb, just like water flows through a tube.",
"batteries have chemical energe in it. then electrons flow through a bulb to light it up.",
"birds can fly because they have feather and they are light.", "why some birds like pigeon can fly while some others like chicken cannot?",
"feather is important for birds' fly. if feather on a bird's wings is removed, this bird cannot fly.")
view <- factor(rep(c("view 1", "view 2", "view 3"), each = 3))
df <- data.frame(text, view, stringsAsFactors = FALSE)
View(df)
external.frame <-data.frame(post= external1.frame$store)
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='personalcorpus', host='localhost')
query<-paste('SELECT store,site,date FROM store')
external1.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
external.frame <-data.frame(post= external1.frame$store)
View(external.frame)
external.frame <-data.frame(post= external1.frame$store, poster = "david")
View(external.frame)
merge.frame <- merge(external1.frame, mydata.frame)
mydata.frame <-data.frame(post= data.frame$store, poster = "david")
merge.frame <- merge(external1.frame, mydata.frame)
View(merge.frame)
merge.frame <- merge(external.frame, mydata.frame)
View(merge.frame)
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='personalcorpus', host='localhost')
query<-paste('SELECT store,site,date FROM store')
external1.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
external.frame <-data.frame(post= external1.frame$store, poster = "mark")
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='paddytherabbit', host='localhost')
query<-paste('SELECT post_content as store from wp_posts WHERE post_status = "publish"')
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
mydata.frame <-data.frame(post= data.frame$store, poster = "david")
merge.frame <- merge(external.frame, mydata.frame)
View(merge.frame)
merge.frame <- rbind(external.frame, mydata.frame)
View(merge.frame)
doc <- Corpus(VectorSource(merge.frame$post))
summary(doc)
externaldoc.corpus <-tm_map(doc, removeWords, stopwords("english")) #removes stopwords
externaldoc.corpus <-tm_map(externaldoc.corpus, stripWhitespace) #removes stopwords
externaldoc.corpus <-tm_map(externaldoc.corpus, tolower)
externaldoc.corpus <-tm_map(externaldoc.corpus, removeNumbers)
externaldoc.corpus <-tm_map(externaldoc.corpus, removePunctuation)
# 2. MDS with raw term-document matrix compute distance matrix
td.mat <- as.matrix(TermDocumentMatrix(externaldoc.corpus ))
dist.mat <- dist(t(as.matrix(td.mat)))
dist.mat  # check distance matrix
fit <- cmdscale(dist.mat, eig = TRUE, k = 2)
points <- data.frame(x = fit$points[, 1], y = fit$points[, 2])
ggplot(points, aes(x = x, y = y)) + geom_point(data = points, aes(x = x, y = y,
color = df$view)) + geom_text(data = points, aes(x = x, y = y - 0.2, label = row.names(df)))
fit <- cmdscale(dist.mat, eig = TRUE, k = 2)
points <- data.frame(x = fit$points[, 1], y = fit$points[, 2])
ggplot(points, aes(x = x, y = y)) + geom_point(data = points, aes(x = x, y = y,
color = merge$view)) + geom_text(data = points, aes(x = x, y = y - 0.2, label = row.names(df)))
fit <- cmdscale(dist.mat, eig = TRUE, k = 2)
points <- data.frame(x = fit$points[, 1], y = fit$points[, 2])
ggplot(points, aes(x = x, y = y)) + geom_point(data = points, aes(x = x, y = y,
color = merge.frame$view)) + geom_text(data = points, aes(x = x, y = y - 0.2, label = row.names(df)))
fit <- cmdscale(dist.mat, eig = TRUE, k = 2)
points <- data.frame(x = fit$points[, 1], y = fit$points[, 2])
ggplot(points, aes(x = x, y = y)) + geom_point(data = points, aes(x = x, y = y,
color = merge.frame$view)) + geom_text(data = points, aes(x = x, y = y - 0.2, label = row.names(merge.frame)))
View(merge.frame)
fit <- cmdscale(dist.mat, eig = TRUE, k = 2)
points <- data.frame(x = fit$points[, 1], y = fit$points[, 2])
ggplot(points, aes(x = x, y = y)) + geom_point(data = points, aes(x = x, y = y,
color = merge.frame$poster)) + geom_text(data = points, aes(x = x, y = y - 0.2, label = row.names(merge.frame)))
library(scatterplot3d)
fit <- cmdscale(dist.mat.lsa, eig = TRUE, k = 3)
colors <- rep(c("blue", "green", "red"), each = 3)
scatterplot3d(fit$points[, 1], fit$points[, 2], fit$points[, 3], color = colors,
pch = 16, main = "Semantic Space Scaled to 3D", xlab = "x", ylab = "y",
zlab = "z", type = "h")
install.packages("scatterplot3d")
library("scatterplot3d", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
fit <- cmdscale(dist.mat.lsa, eig = TRUE, k = 3)
colors <- rep(c("blue", "green", "red"), each = 3)
scatterplot3d(fit$points[, 1], fit$points[, 2], fit$points[, 3], color = colors,
pch = 16, main = "Semantic Space Scaled to 3D", xlab = "x", ylab = "y",
zlab = "z", type = "h")
library(scatterplot3d)
fit <- cmdscale(dist.mat.lsa, eig = TRUE, k = 3)
colors <- rep(c("blue", "green", "red"), each = 3)
scatterplot3d(fit$points[, 1], fit$points[, 2], color = colors,
pch = 16, main = "Semantic Space Scaled to 3D", xlab = "x", ylab = "y",
zlab = "z", type = "h")
library(scatterplot3d)
fit <- cmdscale(dist.mat.lsa, eig = TRUE, k = 3)
colors <- rep(c("red", "green"), each = 3)
scatterplot3d(fit$points[, 1], fit$points[, 2], color = colors,
pch = 16, main = "Semantic Space Scaled to 3D", xlab = "x", ylab = "y",
zlab = "z", type = "h")
zlab = "z", type = "h")
colors <- rep(c("red", "green"), each = 3)
scatterplot3d(fit$points[, 1], fit$points[, 2], color = colors,
pch = 16, main = "Semantic Space Scaled to 3D", xlab = "x", ylab = "y",
zlab = "z", type = "h")
colors <- rep(c("red", "green"), each = 2)
scatterplot3d(fit$points[, 1], fit$points[, 2], color = colors,
pch = 16, main = "Semantic Space Scaled to 3D", xlab = "x", ylab = "y",
zlab = "z", type = "h")
zlab = "z", type = "h")
zlab = "z", type = "h")
fit <- cmdscale(dist.mat.lsa, eig = TRUE, k = 2)
colors <- rep(c("red", "green"), each = 2)
scatterplot3d(fit$points[, 1], fit$points[, 2], color = colors,
pch = 16, main = "Semantic Space Scaled to 3D", xlab = "x", ylab = "y",
zlab = "z", type = "h")
zlab = "z", type = "h")
td.mat.lsa <- lw_bintf(td.mat) * gw_idf(td.mat)  # weighting
lsaSpace <- lsa(td.mat.lsa)  # create LSA space
dist.mat.lsa <- dist(t(as.textmatrix(lsaSpace)))  # compute distance matrix
dist.mat.lsa  # check distance mantrix
ggplot(points, aes(x = x, y = y)) + geom_point(data = points, aes(x = x, y = y,
color = merge.frame$view)) + geom_text(data = points, aes(x = x, y = y - 0.2, label = row.names(merge.frame)))
fit <- cmdscale(dist.mat.lsa, eig = TRUE, k = 2)
points <- data.frame(x = fit$points[, 1], y = fit$points[, 2])
ggplot(points, aes(x = x, y = y)) + geom_point(data = points, aes(x = x, y = y,
color = merge.frame$poster)) + geom_text(data = points, aes(x = x, y = y - 0.2, label = row.names(merge.frame)))
fit <- cmdscale(dist.mat.lsa, eig = TRUE, k = 2)
points <- data.frame(x = fit$points[, 1], y = fit$points[, 2])
ggplot(points, aes(x = x, y = y)) + geom_point(data = points, aes(x = x, y = y,
color = merge.frame$poster)) + geom_text(data = points, aes(x = x, y = y - 0.2))
fit <- cmdscale(dist.mat.lsa, eig = TRUE, k = 2)
points <- data.frame(x = fit$points[, 1], y = fit$points[, 2])
ggplot(points, aes(x = x, y = y)) + geom_point(data = points, aes(x = x, y = y,
color = merge.frame$poster)) ))
fit <- cmdscale(dist.mat.lsa, eig = TRUE, k = 2)
points <- data.frame(x = fit$points[, 1], y = fit$points[, 2])
ggplot(points, aes(x = x, y = y)) + geom_point(data = points, aes(x = x, y = y,
color = merge.frame$poster)) )
fit <- cmdscale(dist.mat.lsa, eig = TRUE, k = 2)
points <- data.frame(x = fit$points[, 1], y = fit$points[, 2])
ggplot(points, aes(x = x, y = y)) + geom_point(data = points, aes(x = x, y = y,
color = merge.frame$poster) )
library(scatterplot3d)
fit <- cmdscale(dist.mat.lsa, eig = TRUE, k = 2)
colors <- rep(c("red", "green"), each = 2)
scatterplot3d(fit$points[, 1], fit$points[, 2], color = colors,
pch = 16, main = "Semantic Space Scaled to 3D", xlab = "x", ylab = "y",
zlab = "z", type = "h")
zlab = "z", type = "h")
fix(colors)
fit <- cmdscale(dist.mat.lsa, eig = TRUE, k = 2)
colors <- rep(c("red", "green"), each = 2)
scatterplot3d(fit$points[, 1], fit$points[, 2], color = colors,
pch = 16, main = "Semantic Space Scaled to 3D", xlab = "x", ylab = "y",
zlab = "z", type = "h")
doc <- Corpus(VectorSource(data.frame$store))
summary(doc)
doc.corpus <-tm_map(doc, removeWords, stopwords("english")) #removes stopwords
doc.corpus <-tm_map(doc.corpus, stripWhitespace) #removes stopwords
doc.corpus <-tm_map(doc.corpus, tolower)
doc.corpus <-tm_map(doc.corpus, removeNumbers)
doc.corpus <-tm_map(doc.corpus, removePunctuation)
library(SnowballC)
doc.corpus <- tm_map(doc.corpus, stemDocument) #TO STEM OR NOT TO STEM
TDM <- TermDocumentMatrix(doc.corpus)
TDM
inspect(TDM[1:10,1:10])
findFreqTerms(TDM, 40)
findAssocs(TDM, "think", 0.8)
TDM.common = removeSparseTerms(TDM, 0.8)
dim(TDM)
dim(TDM.common)
library(slam)
TDM.dense <- as.matrix(TDM.common)
TDM.dense
object.size(TDM.common)
object.size(TDM.dense)
library(reshape2)
TDM.dense = melt(TDM.dense, value.name = "count")
head(TDM.dense)
library(ggplot2)
ggplot(TDM.dense, aes(x = Docs, y = Terms, fill = count)) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
ylab("") +
theme(panel.background = element_blank()) +
theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
documents <- data.frame( text = data.frame$store, stringsAsFactors=FALSE)
documents$id <- Sys.time() + 30 * (seq_len(nrow(documents))-1)
require(mallet)
mallet.instances <- mallet.import( documents$text ,  documents$text , "stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
## Create a topic trainer object.
n.topics <- 30
topic.model <- MalletLDA(n.topics)
#loading the documents
topic.model$loadDocuments(mallet.instances)
## Get the vocabulary, and some statistics about word frequencies.
##  These may be useful in further curating the stopword list.
vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)
## Optimize hyperparameters every 20 iterations,
##  after 50 burn-in iterations.
topic.model$setAlphaOptimization(20, 50)
## Now train a model. Note that hyperparameter optimization is on, by default.
##  We can specify the number of iterations. Here we'll use a large-ish round number.
topic.model$train(200)
## NEW: run through a few iterations where we pick the best topic for each token,
##  rather than sampling from the posterior distribution.
topic.model$maximize(10)
## Get the probability of topics in documents and the probability of words in topics.
## By default, these functions return raw word counts. Here we want probabilities,
##  so we normalize, and add "smoothing" so that nothing has exactly 0 probability.
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)
# from http://www.cs.princeton.edu/~mimno/R/clustertrees.R
## transpose and normalize the doc topics
topic.docs <- t(doc.topics)
topic.docs <- topic.docs / rowSums(topic.docs)
## Get a vector containing short names for the topics
topics.labels <- rep("", n.topics)
for (topic in 1:n.topics) topics.labels[topic] <- paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=" ")
# have a look at keywords for each topic
topicsforsubreddit<-topics.labels
# create data.frame with columns as authors and rows as topics
topic_docs <- data.frame(topic.docs)
names(topic_docs) <- documents$id
topics.labels
documents <- data.frame( text = data.frame$store, stringsAsFactors=FALSE)
documents$id <- Sys.time() + 30 * (seq_len(nrow(documents))-1)
require(mallet)
mallet.instances <- mallet.import( documents$text ,  documents$text , "stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
## Create a topic trainer object.
n.topics <- 15
topic.model <- MalletLDA(n.topics)
#loading the documents
topic.model$loadDocuments(mallet.instances)
## Get the vocabulary, and some statistics about word frequencies.
##  These may be useful in further curating the stopword list.
vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)
## Optimize hyperparameters every 20 iterations,
##  after 50 burn-in iterations.
topic.model$setAlphaOptimization(20, 50)
## Now train a model. Note that hyperparameter optimization is on, by default.
##  We can specify the number of iterations. Here we'll use a large-ish round number.
topic.model$train(200)
## NEW: run through a few iterations where we pick the best topic for each token,
##  rather than sampling from the posterior distribution.
topic.model$maximize(10)
## Get the probability of topics in documents and the probability of words in topics.
## By default, these functions return raw word counts. Here we want probabilities,
##  so we normalize, and add "smoothing" so that nothing has exactly 0 probability.
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)
# from http://www.cs.princeton.edu/~mimno/R/clustertrees.R
## transpose and normalize the doc topics
topic.docs <- t(doc.topics)
topic.docs <- topic.docs / rowSums(topic.docs)
## Get a vector containing short names for the topics
topics.labels <- rep("", n.topics)
for (topic in 1:n.topics) topics.labels[topic] <- paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=" ")
# have a look at keywords for each topic
topicsforsubreddit<-topics.labels
# create data.frame with columns as authors and rows as topics
topic_docs <- data.frame(topic.docs)
names(topic_docs) <- documents$id
topics.labels
documents <- data.frame( text = data.frame$store, stringsAsFactors=FALSE)
documents$id <- Sys.time() + 30 * (seq_len(nrow(documents))-1)
require(mallet)
mallet.instances <- mallet.import( documents$text ,  documents$text , "stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
## Create a topic trainer object.
n.topics <- 5
topic.model <- MalletLDA(n.topics)
#loading the documents
topic.model$loadDocuments(mallet.instances)
## Get the vocabulary, and some statistics about word frequencies.
##  These may be useful in further curating the stopword list.
vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)
## Optimize hyperparameters every 20 iterations,
##  after 50 burn-in iterations.
topic.model$setAlphaOptimization(20, 50)
## Now train a model. Note that hyperparameter optimization is on, by default.
##  We can specify the number of iterations. Here we'll use a large-ish round number.
topic.model$train(200)
## NEW: run through a few iterations where we pick the best topic for each token,
##  rather than sampling from the posterior distribution.
topic.model$maximize(10)
## Get the probability of topics in documents and the probability of words in topics.
## By default, these functions return raw word counts. Here we want probabilities,
##  so we normalize, and add "smoothing" so that nothing has exactly 0 probability.
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)
# from http://www.cs.princeton.edu/~mimno/R/clustertrees.R
## transpose and normalize the doc topics
topic.docs <- t(doc.topics)
topic.docs <- topic.docs / rowSums(topic.docs)
## Get a vector containing short names for the topics
topics.labels <- rep("", n.topics)
for (topic in 1:n.topics) topics.labels[topic] <- paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=" ")
# have a look at keywords for each topic
topicsforsubreddit<-topics.labels
# create data.frame with columns as authors and rows as topics
topic_docs <- data.frame(topic.docs)
names(topic_docs) <- documents$id
topics.labels
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='cetis_blogs_new', host='localhost')
query<-paste('SELECT post_content FROM cetiswp_5_posts where post_status = "publish" LIMIT 50')
sheila.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
mydata.frame <-data.frame(post= sheila1.frame$store, poster = "sheila")
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='cetis_blogs_new', host='localhost')
query<-paste('SELECT post_contentas store FROM cetiswp_5_posts where post_status = "publish" LIMIT 50')
sheila1.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='cetis_blogs_new', host='localhost')
query<-paste('SELECT post_content as store FROM cetiswp_5_posts where post_status = "publish" LIMIT 50')
sheila1.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
mydata.frame <-data.frame(post= sheila1.frame$store, poster = "sheila")
merge.frame <- rbind(external.frame, mydata.frame, sheila.frame)
View(sheila.frame)
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='personalcorpus', host='localhost')
query<-paste('SELECT store,site,date FROM store')
external1.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
external.frame <-data.frame(post= external1.frame$store, poster = "mark")
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='paddytherabbit', host='localhost')
query<-paste('SELECT post_content as store from wp_posts WHERE post_status = "publish"')
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
mydata.frame <-data.frame(post= data.frame$store, poster = "david")
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='cetis_blogs_new', host='localhost')
query<-paste('SELECT post_content as store FROM cetiswp_5_posts where post_status = "publish" LIMIT 50')
sheila1.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
sheila.frame <-data.frame(post= sheila1.frame$store, poster = "sheila")
merge.frame <- rbind(external.frame, mydata.frame, sheila.frame)
doc <- Corpus(VectorSource(merge.frame$post))
summary(doc)
externaldoc.corpus <-tm_map(doc, removeWords, stopwords("english")) #removes stopwords
externaldoc.corpus <-tm_map(externaldoc.corpus, stripWhitespace) #removes stopwords
externaldoc.corpus <-tm_map(externaldoc.corpus, tolower)
externaldoc.corpus <-tm_map(externaldoc.corpus, removeNumbers)
externaldoc.corpus <-tm_map(externaldoc.corpus, removePunctuation)
# 2. MDS with raw term-document matrix compute distance matrix
td.mat <- as.matrix(TermDocumentMatrix(externaldoc.corpus ))
dist.mat <- dist(t(as.matrix(td.mat)))
dist.mat  # check distance matrix
fit <- cmdscale(dist.mat, eig = TRUE, k = 2)
points <- data.frame(x = fit$points[, 1], y = fit$points[, 2])
ggplot(points, aes(x = x, y = y)) + geom_point(data = points, aes(x = x, y = y,
color = merge.frame$poster)) + geom_text(data = points, aes(x = x, y = y - 0.2, label = row.names(merge.frame)))
# 3. MDS with LSA
td.mat.lsa <- lw_bintf(td.mat) * gw_idf(td.mat)  # weighting
lsaSpace <- lsa(td.mat.lsa)  # create LSA space
dist.mat.lsa <- dist(t(as.textmatrix(lsaSpace)))  # compute distance matrix
dist.mat.lsa  # check distance mantrix
fit <- cmdscale(dist.mat.lsa, eig = TRUE, k = 2)
points <- data.frame(x = fit$points[, 1], y = fit$points[, 2])
ggplot(points, aes(x = x, y = y)) + geom_point(data = points, aes(x = x, y = y,
color = merge.frame$poster)) + geom_text(data = points, aes(x = x, y = y - 0.2, label = row.names(merge.frame)))
fit <- cmdscale(dist.mat.lsa, eig = TRUE, k = 2)
points <- data.frame(x = fit$points[, 1], y = fit$points[, 2])
ggplot(points, aes(x = x, y = y)) + geom_point(data = points, aes(x = x, y = y,
color = merge.frame$poster) )
View(data.frame)
documents
documents$id <- Sys.time() + 30 * (seq_len(nrow(documents))-1)
source('~/Desktop/RStudio/Personal Corpus/R/blog_wordpress_mysql.R')
View(data.frame)
documents <- data.frame( text = data.frame$store, stringsAsFactors=FALSE)
documents$id <- Sys.time() + 30 * (seq_len(nrow(documents))-1)
View(documents)
require(mallet)
mallet.instances <- mallet.import( documents$text ,  documents$text , "stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
## Create a topic trainer object.
n.topics <- 5
topic.model <- MalletLDA(n.topics)
#loading the documents
fix(topic.model)
source('~/Desktop/RStudio/Personal Corpus/R/blog_topicmodels.R')
topics.labels
fix(vocabulary)
View(word.freqs)
topic.model
doc.topics
df<-read.csv("/Users/David/Desktop/cybernetics.csv" , header=T, sep=",")
M = as.matrix( table(df) )
Mrow = M %*% t(M)
View(Mrow)
iMrow = graph.adjacency(Mrow, mode = "undirected")
E(iMrow)$weight <- count.multiple(iMrow)
library("igraph", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
iMrow <- simplify(iMrow)
iMrow = graph.adjacency(Mrow, mode = "undirected")
E(iMrow)$weight <- count.multiple(iMrow)
iMrow <- simplify(iMrow)
write.graph(iMrow, file="graph.graphml", format="graphml");
